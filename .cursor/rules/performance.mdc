# Performance & Large Dataset Rules

## Memory Efficiency
- **Avoid unnecessary copies** of large arrays
- **Use views** when possible (`arr[1:]` not `arr[1:].copy()`)
- **Process data in chunks** for very large datasets
- **Monitor memory usage** in long-running processes

## Performance Monitoring for Large Datasets
- **Memory profiling**: Use `memory_profiler` for datasets with 1000+ flies
- **Progress tracking**: Log processing milestones for long runs
- **Checkpoint system**: Save intermediate results for recovery
- **Resource limits**: Monitor CPU/memory usage in Colab
- **Batch processing**: Process flies in manageable chunks

## Computational Efficiency
- **Profile before optimizing** - measure first
- **Use appropriate data types** for the task
- **Consider caching** for expensive computations
- **Document performance characteristics**

## Vectorization Guidelines
- **Prefer NumPy operations** over Python loops
- **Use broadcasting** for efficient array operations
- **Document performance considerations** in docstrings
- **Profile critical sections** for optimization
- **Vectorization preference**: Prefer NumPy/Pandas vectorization; loops only if vectorization hurts readability

## Large Dataset Handling (1000+ flies)
- **Memory profiling**: Use `memory_profiler` for datasets with 1000+ flies
- **Progress tracking**: Log processing milestones for long runs
- **Checkpoint system**: Save intermediate results for recovery
- **Resource limits**: Monitor CPU/memory usage in Colab
- **Batch processing**: Process flies in manageable chunks

## Performance Anti-Patterns
❌ **Unnecessary loops** when vectorization is possible
❌ **Memory leaks** from unclosed resources
❌ **Inefficient data structures** for the use case
❌ **Blocking operations** in main thread
❌ **Excessive memory usage** for large datasets

## Performance Examples
```python
# Good: Use views instead of copies
def process_large_array(data: np.ndarray) -> np.ndarray:
    """Process large array efficiently."""
    # Use view instead of copy
    processed = data[1:] - data[:-1]  # Good: view
    # processed = data[1:].copy() - data[:-1].copy()  # Bad: unnecessary copies
    return processed

# Good: Batch processing for large datasets
def process_flies_in_batches(fly_data: list, batch_size: int = 100) -> None:
    """Process flies in manageable batches."""
    for i in range(0, len(fly_data), batch_size):
        batch = fly_data[i:i + batch_size]
        process_batch(batch)
        # Save checkpoint
        save_checkpoint(i + batch_size)

# Good: Memory monitoring
import psutil

def monitor_memory_usage() -> None:
    """Monitor memory usage during processing."""
    memory_percent = psutil.virtual_memory().percent
    if memory_percent > 80:
        logger.warning(f"High memory usage: {memory_percent}%")
        # Implement memory cleanup
```

## Colab Environment Performance
- **Memory management**: Monitor memory usage for large datasets (1000+ flies)
- **Progress indicators**: Use `tqdm` for long-running operations
- **Error recovery**: Implement checkpoint/resume for long processes
- **Session persistence**: Save intermediate results to prevent data loss
- **Resource limits**: Monitor CPU/memory usage in Colab
